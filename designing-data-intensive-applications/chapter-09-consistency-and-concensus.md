# Chapter 9. Consistency and Consensus

- Distributed systems can fail in many ways (lost, delayed, or duplicated messages; clock issues; node pauses or crashes).
- Instead of letting services fail, fault tolerance aims to keep them functioning despite such problems.
- The best approach is to build **general-purpose abstractions** ‚Äî like **transactions** or **consensus** ‚Äî that hide failures from applications.
- Consensus, in particular, ensures **all nodes agree on decisions** (e.g., leader election), preventing issues like **split brain**.
- This chapter explores the guarantees and abstractions distributed systems can provide, along with their fundamental limits, offering an informal overview while pointing to deeper research in the literature.

## Consistency Guarantees

- Replication in databases leads to temporary inconsistencies between nodes, regardless of replication method. Most systems offer **eventual consistency** (better termed **convergence**), meaning replicas will agree eventually, but without guarantees on when.
- This weak model complicates application development, since it differs from the predictable behavior of variables in single-threaded programs. Subtle bugs often appear only under faults or high concurrency.
- The chapter explores **stronger consistency models**, which trade performance and fault tolerance for easier correctness. It introduces:
  - **Linearizability** (a strong model) and its pros/cons.
  - **Ordering guarantees** (causality, total order).
  - **Distributed transactions** and **consensus**, focusing on atomic commits.
- Consistency models, while related to transaction isolation levels, address different challenges: replica coordination vs. concurrency control.

## Linearizability

- Linearizability (also called **strong** or **atomic consistency**) is the guarantee that a distributed system behaves as if there were only **one copy of the data**. Every operation appears instantaneous and immediately visible to all clients.
- In a linearizable system, once a **write completes**, all subsequent reads return the **updated value** (no stale data).
- It provides a **recency guarantee**: reads always reflect the most recent completed write.
- Without linearizability (e.g., in **eventually consistent** systems), different replicas may return different answers, leading to anomalies.
- üëâ After Alice sees the final score of a match, Bob refreshes later but still sees an outdated result ‚Äî a clear violation of linearizability.

### What Makes a System Linearizable?

- **Reads and Writes**:
  - Reads that happen strictly before a write must return the old value.
  - Reads that happen strictly after a write must return the new value.
  - Reads concurrent with a write may return either, but once one client sees the new value, all later reads must also return it (no ‚Äúflip-flopping‚Äù).
- **Atomicity model**:
  - Each operation is pictured as a bar (request to response) with a point inside representing the moment it logically takes effect.
  - The sequence of these points must form a valid forward-moving history: once a value is written or read, all subsequent reads reflect it until overwritten again.
- **Concurrency quirks**:
  - Operations may complete in an **order different** from request arrival, and that‚Äôs okay as long as the resulting sequence is valid.
  - Responses can be **delayed**, so a read might return a new value before the writer has even received its own success acknowledgment.
  - CAS helps ensure updates aren‚Äôt lost to concurrent modifications.
- **Violations**:
  - If a client reads an older value after another client has already read a newer one, linearizability is broken (similar to the Alice & Bob example).
- **Testing linearizability**:
  - You can verify it (at high cost) by recording request/response timings and checking if operations can be ordered into a valid sequential history.
- üëâ Linearizability = the illusion of a single, atomic timeline of operations, preserving recency guarantees across concurrent clients.

#### Linearizability Versus Serializability

- TThe key difference between **linearizability** and **serializability** is often confused because both involve the idea of a **sequential order**. However, they are distinct guarantees:
  - Serializability is an **isolation property** for **transactions** (groups of operations). It guarantees that the result of multiple concurrent transactions is equivalent to if they had executed one after another (in some serial order). It is about the correctness of a group of operations.
  - Linearizability is a **recency guarantee** for a single object. It ensures that once a write completes, all subsequent reads will see that value until it is overwritten. It is about the freshness of a single operation.
A system can be:
  - Both: This combination is called **strict serializability**.
  - Serializable but not linearizable: For example, databases using SSI use consistent snapshots, so reads may not see the very latest write.
  - Linearizable but not serializable: A system can guarantee fresh reads on individual objects but not protect against multi-object transaction anomalies like **write skew**.
- üëâ Serializability is about transactions appearing atomic; linearizability is about reads and writes appearing instantaneous.

### Relying on Linearizability

- Linearizability is essential in distributed systems for three main use cases where strict agreement on the most recent state of a single object is necessary:
  - **Locking and Leader Election**: Systems that use a distributed lock or single-leader replication must have a linearizable lock to prevent **split-brain** scenarios. All nodes must agree on which node currently holds the lock. Coordination services like `ZooKeeper` and `etcd` provide this linearizable foundation.
  - **Enforcing Uniqueness Constraints**: Hard constraints, such as ensuring a username or email address is unique, require linearizability. This is effectively like acquiring a lock on a value. If two users try to create the same username concurrently, a linearizable system guarantees that only one will succeed. This also applies to constraints like preventing a bank account from going negative.
  - **Cross-Channel Timing Dependencies**: Linearizability prevents race conditions when two different communication channels depend on the same data. For example, if a web server writes a photo to storage (channel 1) and then sends a message via a queue (channel 2) for another service to process it, a non-linearizable storage service might cause the processor to read a stale or missing file, leading to inconsistencies.

### Implementing Linearizable Systems

- The naive way is a single copy of data, but that isn‚Äôt fault-tolerant ü§∑.
- So, replication methods are used ‚Äî but not all support linearizability:
- **Single-leader replication**:
  - Can be linearizable if **reads** go to the **leader** or **synchronously** updated followers.
  - Risks: leader uncertainty (split brain), async replication causing lost writes, or databases intentionally using weaker models.
- **Consensus algorithms** (e.g., `Raft`, `Paxos`, `ZooKeeper`, `etcd`)
  - Provide linearizability by preventing split brain, ensuring durability, and keeping replicas consistent.
  - Safest way to implement linearizable storage üëç.
- **Multi-leader replication**:
  - Not linearizable: multiple leaders accept concurrent writes ‚Üí conflicts, async replication ‚Üí no single-copy illusion.
- **Leaderless replication** (`Dynamo`-style):
  - Often claimed to be ‚Äústrong‚Äù with quorum reads/writes (`w + r > n`), but still not strictly linearizable.
  - LWW with clocks ‚Üí broken by **clock skew**.
  - **Sloppy quorums/hinted handoff** break linearizability further.
  - Even strict quorums can allow anomalies.

#### Linearizability and quorums

- Quorums (`r + w > n`) in `Dynamo`-style systems may seem to guarantee linearizability, but network delays can cause race conditions.
- Example:
  - Writer updates `x = 1` across 3 replicas (w = 3).
  - Reader A (r = 2) sees `1`.
  - Later, Reader B (r = 2) still sees 0.
  - Even though quorum conditions are satisfied, this violates linearizability.
- To achieve linearizability:
  - Readers must perform **synchronous read repair**.
  - Writers must read the latest quorum state before writing.
- ‚öñÔ∏è:
  - Riak skips synchronous read repair ‚Üí better performance, but no linearizability.
  - `Cassandra` does synchronous repair for quorum reads, but still breaks linearizability under concurrent writes (LWW).
- Limitation: Only reads and writes can be made linearizable this way ‚Äî CAS requires full consensus.
- Conclusion: Dynamo-style leaderless replication should be assumed not linearizable.

### The Cost of Linearizability

- Multi-leader replication:
  - Each datacenter can keep accepting writes independently.
  - Writes are queued and later exchanged once connectivity is restored.
  - System stays available, but may have conflicts to resolve later.
- Single-leader replication:
  - All writes and linearizable reads must go through the leader.
  - If the leader‚Äôs DC is unreachable, follower DC clients cannot perform writes or linearizable reads (only stale reads).
  - This means **outages** for clients that can‚Äôt reach the leader until the network recovers.
- ‚öñÔ∏è Multi-leader offers higher availability under network partitions, while single-leader ensures strict consistency but risks unavailability.

#### The CAP theorem

- Linearizable systems: If replicas are disconnected, they cannot safely process requests ‚Üí they must block or return errors ‚Üí unavailability.
- Non-linearizable systems (e.g., multi-leader): Replicas can keep working independently during disconnections ‚Üí higher availability, but weaker consistency.
- This trade-off is the essence of the CAP theorem: during a **network partition**, you must choose between **Consistency** (linearizability) and **Availability**.
- CAP is often misrepresented as ‚Äú*pick 2 of 3,*‚Äù but the real meaning is:
  - In normal conditions: you can have both consistency + availability.
  - Under partition: you must sacrifice one.
- CAP was influential (helped inspire `NoSQL` systems), but:
  - It only covers linearizability + partitions, not **other faults** (delays, crashes, etc.).
  - Its definitions of availability are confusing ü§∑‚Äç‚ôÄÔ∏è.
- It has limited practical value today, mostly of historical interest, as more precise results now exist.

#### Linearizability and network delays

- Few systems are truly linearizable ‚Äî even RAM on modern **multi-core CPUs** isn‚Äôt.
  - Each CPU core has caches and store buffers. Writes are propagated asynchronously to main memory.
  - This boosts performance but breaks linearizability unless memory **fences** are used.
- Reason for dropping linearizability:
  - Not CAP or fault tolerance.
  - Purely performance ‚Äî linearizability is always slower, not just during faults.
- Distributed databases often avoid linearizability for the same reason: to improve speed and latency.
- Theory (*Attiya & Welch*):
  - Linearizable storage is fundamentally slow.
  - Response times are at least proportional to network delay uncertainty.
  - No faster algorithm exists for linearizability.
- ‚öñÔ∏è:
  - Linearizability = correctness but high latency.
  - Weaker consistency = much faster, better for latency-sensitive systems.

## Ordering Guarantees

- Linearizability ensures operations appear to occur atomically in a **single global order**.
- Ordering recurs in many contexts:
  - **Leaders** in **replication logs** impose order on writes to avoid conflicts.
  - **Serializability** ensures transactions act as if executed in sequential order.
  - **Timestamps/clocks** help determine event order in distributed settings.
- The link between **ordering** and **causality** is üîë:
  - **Causality** means causes must precede effects (question before answer, create before update, message sent before received).
  - Violations of causality cause anomalies like:
    - Reading answers before questions (prefix inconsistency).
    - Updates to nonexistent rows due to overtaking writes.
    - Non-repeatable reads that show effects without their causes.
    - Write skew where decisions depend on outdated assumptions.
    - Observing stale data even after effects are already visible elsewhere.
- **Causal consistency** requires that if you see some data, you must also see all data that causally precedes it. For example, snapshot isolation guarantees causal consistency by ensuring snapshots reflect all causally prior operations.
- üëâ Overall: Ordering is fundamental in distributed systems because it preserves causality, which underpins intuitive correctness (cause-before-effect).

### The causal order is not a total order

- **Total order**: Any two elements can always be compared (e.g., numbers).
- **Partial order**: Some elements can be compared, others are incomparable (e.g., sets where neither is a subset of the other).
- Applied to databases and consistency models:
  - **Linearizability** = total order of operations.
    - Every operation appears atomic.
    - All operations fall on a single global timeline.
    - No concurrency in the logical model: one operation always comes before the other.
  - **Causality** = partial order of operations.
    - Operations are ordered only if causally related (one depends on the other).
    - Concurrent operations are incomparable (neither before the other).
    - This results in branching timelines that later may merge, as seen in distributed systems.
- Analogy: Distributed version control (e.g., `Git`).
  - Linear history = total order.
  - Branches and merges = partial order (causal graph of commits).
- üëâ Key point: Linearizability enforces a single, strict timeline (total order), while causality reflects the reality of concurrency (partial order with branching and merging).

### Linearizability is stronger than causal consistency

- Linearizability ‚ñ∂Ô∏è causality.
- A linearizable system automatically preserves causal relationships, even across multiple communication channels.
- üëç of linearizability:
  - Simple to reason about.
  - Intuitive model (operations appear atomic in one timeline).
- üëé of linearizability:
  - Hurts performance and availability, especially with high network latency or geo-distribution.
- Middle ground:
  - Causal consistency also preserves causality, but without the performance/availability penalties of linearizability.
  - Strongest consistency model that is still tolerant of network delays/failures (not limited by CAP theorem).
- Practical insight:
  - Many systems don‚Äôt really need **full** linearizability‚Äî **causal** consistency is often enough.
  - Research is exploring efficient databases that offer causal consistency with performance close to eventual consistency.
- Current state:
  - Still experimental and not widely in production.
  - Promising direction, but with open challenges.
- üëâ Key idea: Linearizability is sufficient but costly; causal consistency is often sufficient and much cheaper.

### Capturing causal dependencies

- To maintain causal consistency, a system must ensure that if one operation happened before another, replicas process them in that order. Concurrent operations can be applied in any order.
- üîë ideas:
  - A replica can only process an operation once all its causally preceding operations are applied; otherwise, it must wait.
  - Determining causality requires tracking what a node ‚Äúknew‚Äù when it issued an operation (similar to tracing dependencies).
  - Techniques resemble detecting concurrent writes in leaderless datastores but extend across the entire database, not just one key.
  - **Version vectors** (generalized) help track causal dependencies.
  - Databases often need to know what version of data was read before a write‚Äîpassing version info back ensures causal order is respected.
  - Similar to SSI, the system checks if data read during a transaction is still current at commit time.
- üëâ Essentially: causal consistency relies on tracking and enforcing the ‚Äúhappens-before‚Äù relationships across all data, often using version vectors and read-dependency tracking.

### Sequence Number Ordering