# Chapter 9. Consistency and Consensus

- Distributed systems can fail in many ways (lost, delayed, or duplicated messages; clock issues; node pauses or crashes).
- Instead of letting services fail, fault tolerance aims to keep them functioning despite such problems.
- The best approach is to build **general-purpose abstractions** â€” like **transactions** or **consensus** â€” that hide failures from applications.
- Consensus, in particular, ensures **all nodes agree on decisions** (e.g., leader election), preventing issues like **split brain**.
- This chapter explores the guarantees and abstractions distributed systems can provide, along with their fundamental limits, offering an informal overview while pointing to deeper research in the literature.

## Consistency Guarantees

- Replication in databases leads to temporary inconsistencies between nodes, regardless of replication method. Most systems offer **eventual consistency** (better termed **convergence**), meaning replicas will agree eventually, but without guarantees on when.
- This weak model complicates application development, since it differs from the predictable behavior of variables in single-threaded programs. Subtle bugs often appear only under faults or high concurrency.
- The chapter explores **stronger consistency models**, which trade performance and fault tolerance for easier correctness. It introduces:
  - **Linearizability** (a strong model) and its pros/cons.
  - **Ordering guarantees** (causality, total order).
  - **Distributed transactions** and **consensus**, focusing on atomic commits.
- Consistency models, while related to transaction isolation levels, address different challenges: replica coordination vs. concurrency control.

## Linearizability

- Linearizability (also called **strong** or **atomic consistency**) is the guarantee that a distributed system behaves as if there were only **one copy of the data**. Every operation appears instantaneous and immediately visible to all clients.
- In a linearizable system, once a **write completes**, all subsequent reads return the **updated value** (no stale data).
- It provides a **recency guarantee**: reads always reflect the most recent completed write.
- Without linearizability (e.g., in **eventually consistent** systems), different replicas may return different answers, leading to anomalies.
- ğŸ‘‰ After Alice sees the final score of a match, Bob refreshes later but still sees an outdated result â€” a clear violation of linearizability.

### What Makes a System Linearizable?

- **Reads and Writes**:
  - Reads that happen strictly before a write must return the old value.
  - Reads that happen strictly after a write must return the new value.
  - Reads concurrent with a write may return either, but once one client sees the new value, all later reads must also return it (no â€œflip-floppingâ€).
- **Atomicity model**:
  - Each operation is pictured as a bar (request to response) with a point inside representing the moment it logically takes effect.
  - The sequence of these points must form a valid forward-moving history: once a value is written or read, all subsequent reads reflect it until overwritten again.
- **Concurrency quirks**:
  - Operations may complete in an **order different** from request arrival, and thatâ€™s okay as long as the resulting sequence is valid.
  - Responses can be **delayed**, so a read might return a new value before the writer has even received its own success acknowledgment.
  - CAS helps ensure updates arenâ€™t lost to concurrent modifications.
- **Violations**:
  - If a client reads an older value after another client has already read a newer one, linearizability is broken (similar to the Alice & Bob example).
- **Testing linearizability**:
  - You can verify it (at high cost) by recording request/response timings and checking if operations can be ordered into a valid sequential history.
- ğŸ‘‰ Linearizability = the illusion of a single, atomic timeline of operations, preserving recency guarantees across concurrent clients.

#### Linearizability Versus Serializability

- The key difference between **linearizability** and **serializability** is often confused because both involve the idea of a **sequential order**. However, they are distinct guarantees:
  - **Serializability** is an **isolation property** for **transactions** (groups of operations). It guarantees that the result of multiple concurrent transactions is equivalent to if they had executed one after another (in some serial order). It is about the correctness of a group of operations.
  - **Linearizability** is a **recency guarantee** for a single object. It ensures that once a write completes, all subsequent reads will see that value until it is overwritten. It is about the freshness of a single operation.
- A system can be:
  - **Both**: This combination is called **strict serializability**.
  - **Serializable but not linearizable**: For example, databases using SSI use consistent snapshots, so reads may not see the very latest write.
  - **Linearizable but not serializable**: A system can guarantee fresh reads on individual objects but not protect against multi-object transaction anomalies like **write skew**.
- ğŸ‘‰ Serializability is about transactions appearing atomic; linearizability is about reads and writes appearing instantaneous.

### Relying on Linearizability

- Linearizability is essential in distributed systems for three main use cases where strict agreement on the most recent state of a single object is necessary:
  - **Locking and Leader Election**: Systems that use a distributed lock or single-leader replication must have a linearizable lock to prevent **split-brain** scenarios. All nodes must agree on which node currently holds the lock. Coordination services like `ZooKeeper` and `etcd` provide this linearizable foundation.
  - **Enforcing Uniqueness Constraints**: Hard constraints, such as ensuring a username or email address is unique, require linearizability. This is effectively like acquiring a lock on a value. If two users try to create the same username concurrently, a linearizable system guarantees that only one will succeed. This also applies to constraints like preventing a bank account from going negative.
  - **Cross-Channel Timing Dependencies**: Linearizability prevents race conditions when two different communication channels depend on the same data. For example, if a web server writes a photo to storage (channel 1) and then sends a message via a queue (channel 2) for another service to process it, a non-linearizable storage service might cause the processor to read a stale or missing file, leading to inconsistencies.

### Implementing Linearizable Systems

- The naive way is a single copy of data, but that isnâ€™t fault-tolerant ğŸ¤·.
- So, replication methods are used â€” but not all support linearizability:
- **Single-leader replication**:
  - Can be linearizable if **reads** go to the **leader** or **synchronously** updated followers.
  - Risks: leader uncertainty (split brain), async replication causing lost writes, or databases intentionally using weaker models.
- **Consensus algorithms** (e.g., `Raft`, `Paxos`, `ZooKeeper`, `etcd`)
  - Provide linearizability by preventing split brain, ensuring durability, and keeping replicas consistent.
  - Safest way to implement linearizable storage ğŸ‘.
- **Multi-leader replication**:
  - Not linearizable: multiple leaders accept concurrent writes â†’ conflicts, async replication â†’ no single-copy illusion.
- **Leaderless replication** (`Dynamo`-style):
  - Often claimed to be â€œstrongâ€ with quorum reads/writes (`w + r > n`), but still not strictly linearizable.
  - LWW with clocks â†’ broken by **clock skew**.
  - **Sloppy quorums/hinted handoff** break linearizability further.
  - Even strict quorums can allow anomalies.

#### Linearizability and quorums

- Quorums (`r + w > n`) in `Dynamo`-style systems may seem to guarantee linearizability, but network delays can cause race conditions.
- Example:
  - Writer updates `x = 1` across 3 replicas (w = 3).
  - Reader A (r = 2) sees `1`.
  - Later, Reader B (r = 2) still sees 0.
  - Even though quorum conditions are satisfied, this violates linearizability.
- To achieve linearizability:
  - Readers must perform **synchronous read repair**.
  - Writers must read the latest quorum state before writing.
- âš–ï¸:
  - Riak skips synchronous read repair â†’ better performance, but no linearizability.
  - `Cassandra` does synchronous repair for quorum reads, but still breaks linearizability under concurrent writes (LWW).
- Limitation: Only reads and writes can be made linearizable this way â€” CAS requires full consensus.
- Conclusion: Dynamo-style leaderless replication should be assumed not linearizable.

### The Cost of Linearizability

- Multi-leader replication:
  - Each datacenter can keep accepting writes independently.
  - Writes are queued and later exchanged once connectivity is restored.
  - System stays available, but may have conflicts to resolve later.
- Single-leader replication:
  - All writes and linearizable reads must go through the leader.
  - If the leaderâ€™s DC is unreachable, follower DC clients cannot perform writes or linearizable reads (only stale reads).
  - This means **outages** for clients that canâ€™t reach the leader until the network recovers.
- âš–ï¸ Multi-leader offers higher availability under network partitions, while single-leader ensures strict consistency but risks unavailability.

#### The CAP theorem

- Linearizable systems: If replicas are disconnected, they cannot safely process requests â†’ they must block or return errors â†’ unavailability.
- Non-linearizable systems (e.g., multi-leader): Replicas can keep working independently during disconnections â†’ higher availability, but weaker consistency.
- This trade-off is the essence of the CAP theorem: during a **network partition**, you must choose between **Consistency** (linearizability) and **Availability**.
- CAP is often misrepresented as â€œ*pick 2 of 3,*â€ but the real meaning is:
  - In normal conditions: you can have both consistency + availability.
  - Under partition: you must sacrifice one.
- CAP was influential (helped inspire `NoSQL` systems), but:
  - It only covers linearizability + partitions, not **other faults** (delays, crashes, etc.).
  - Its definitions of availability are confusing ğŸ¤·â€â™€ï¸.
- It has limited practical value today, mostly of historical interest, as more precise results now exist.

#### Linearizability and network delays

- Few systems are truly linearizable â€” even RAM on modern **multi-core CPUs** isnâ€™t.
  - Each CPU core has caches and store buffers. Writes are propagated asynchronously to main memory.
  - This boosts performance but breaks linearizability unless memory **fences** are used.
- Reason for dropping linearizability:
  - Not CAP or fault tolerance.
  - Purely performance â€” linearizability is always slower, not just during faults.
- Distributed databases often avoid linearizability for the same reason: to improve speed and latency.
- Theory (*Attiya & Welch*):
  - Linearizable storage is fundamentally slow.
  - Response times are at least proportional to network delay uncertainty.
  - No faster algorithm exists for linearizability.
- âš–ï¸:
  - Linearizability = correctness but high latency.
  - Weaker consistency = much faster, better for latency-sensitive systems.

## Ordering Guarantees

- Linearizability ensures operations appear to occur atomically in a **single global order**.
- Ordering recurs in many contexts:
  - **Leaders** in **replication logs** impose order on writes to avoid conflicts.
  - **Serializability** ensures transactions act as if executed in sequential order.
  - **Timestamps/clocks** help determine event order in distributed settings.
- The link between **ordering** and **causality** is ğŸ”‘:
  - **Causality** means causes must precede effects (question before answer, create before update, message sent before received).
  - Violations of causality cause anomalies like:
    - Reading answers before questions (prefix inconsistency).
    - Updates to nonexistent rows due to overtaking writes.
    - Non-repeatable reads that show effects without their causes.
    - Write skew where decisions depend on outdated assumptions.
    - Observing stale data even after effects are already visible elsewhere.
- **Causal consistency** requires that if you see some data, you must also see all data that causally precedes it. For example, snapshot isolation guarantees causal consistency by ensuring snapshots reflect all causally prior operations.
- ğŸ‘‰ Overall: Ordering is fundamental in distributed systems because it preserves causality, which underpins intuitive correctness (cause-before-effect).

### The causal order is not a total order

- **Total order**: Any two elements can always be compared (e.g., numbers).
- **Partial order**: Some elements can be compared, others are incomparable (e.g., sets where neither is a subset of the other).
- Applied to databases and consistency models:
  - **Linearizability** = total order of operations.
    - Every operation appears atomic.
    - All operations fall on a single global timeline.
    - No concurrency in the logical model: one operation always comes before the other.
  - **Causality** = partial order of operations.
    - Operations are ordered only if causally related (one depends on the other).
    - Concurrent operations are incomparable (neither before the other).
    - This results in branching timelines that later may merge, as seen in distributed systems.
- Analogy: Distributed version control (e.g., `Git`).
  - Linear history = total order.
  - Branches and merges = partial order (causal graph of commits).
- ğŸ‘‰ Key point: Linearizability enforces a single, strict timeline (total order), while causality reflects the reality of concurrency (partial order with branching and merging).

### Linearizability is stronger than causal consistency

- Linearizability â–¶ï¸ causality.
- A linearizable system automatically preserves causal relationships, even across multiple communication channels.
- ğŸ‘ of linearizability:
  - Simple to reason about.
  - Intuitive model (operations appear atomic in one timeline).
- ğŸ‘ of linearizability:
  - Hurts performance and availability, especially with high network latency or geo-distribution.
- Middle ground:
  - Causal consistency also preserves causality, but without the performance/availability penalties of linearizability.
  - Strongest consistency model that is still tolerant of network delays/failures (not limited by CAP theorem).
- Practical insight:
  - Many systems donâ€™t really need **full** linearizabilityâ€” **causal** consistency is often enough.
  - Research is exploring efficient databases that offer causal consistency with performance close to eventual consistency.
- Current state:
  - Still experimental and not widely in production.
  - Promising direction, but with open challenges.
- ğŸ‘‰ Key idea: Linearizability is sufficient but costly; causal consistency is often sufficient and much cheaper.

### Capturing causal dependencies

- To maintain causal consistency, a system must ensure that if one operation happened before another, replicas process them in that order. Concurrent operations can be applied in any order.
- ğŸ”‘ ideas:
  - A replica can only process an operation once all its causally preceding operations are applied; otherwise, it must wait.
  - Determining causality requires tracking what a node â€œknewâ€ when it issued an operation (similar to tracing dependencies).
  - Techniques resemble detecting concurrent writes in leaderless datastores but extend across the entire database, not just one key.
  - **Version vectors** (generalized) help track causal dependencies.
  - Databases often need to know what version of data was read before a writeâ€”passing version info back ensures causal order is respected.
  - Similar to SSI, the system checks if data read during a transaction is still current at commit time.
- ğŸ‘‰ Essentially: causal consistency relies on tracking and enforcing the â€œhappens-beforeâ€ relationships across all data, often using version vectors and read-dependency tracking.

### Sequence Number Ordering

Tracking all causal dependencies directly is impractical because clients often read lots of data before writing. Instead, sequence numbers or logical timestamps can efficiently order operations. These are compact, provide a total order of events, and can be made consistent with causality: if A happened before `B`, `A`â€™s number is lower. Concurrent operations can be ordered arbitrarily.

In databases with single-leader replication, the leaderâ€™s log naturally provides this total order â€” each write gets a monotonically increasing sequence number. Followers that apply operations in log order remain causally consistent, even if they lag behind.

#### Noncausal sequence number generators

- In **multi-leader** or **leaderless** databases, generating sequence numbers is harder. Common approaches include:
  - Per-node counters (e.g., odd/even, or embedding node IDs in the number).
  - Physical clock timestamps (time-of-day).
  - Preallocated blocks of sequence numbers per node.
- These methods scale better than using a single leader, ensuring unique and roughly increasing numbers. However, none preserve **causal consistency**, since:
  - Nodes may progress at different speeds (odd/even scheme).
  - Physical clocks suffer from skew, misordering events.
  - Block allocation may assign lower numbers to later operations.
- ğŸ‘‰ In short: they provide uniqueness and scalability, but not causal ordering.

#### Lamport timestamps

- Lamport timestamps (1978) provide a **causality-consistent** way of generating sequence numbers.
- Each node maintains a counter and pairs it with its node `ID â†’ (counter, nodeID)`. Ordering works by comparing counters first, then node IDs if counters are equal.
- The key idea: every node/client tracks the **maximum** counter value seen and includes it in requests. If a node sees a higher counter than its own, it **jumps forward**. This guarantees that causal dependencies always result in higher timestamps.
- Unlike **version vectors**, Lamport timestamps cannot distinguish between **concurrency** and **dependency**, since they enforce a total order of all operations. Their advantage is **compactness**.

#### Timestamp ordering is not sufficient

- Lamport timestamps provide a **causality-consistent** total order, but they are not enough for problems like enforcing **uniqueness** constraints (e.g., usernames).
- While you can resolve conflicts after the fact by comparing timestamps, you canâ€™t decide safely in real timeâ€”because **other nodes** may **concurrently** create operations with earlier timestamps that you donâ€™t yet know about ğŸ¤·.
- Thus, Lamport timestamps only define order after **all operations are known**, but not when that order is finalized.
- To guarantee correctness (e.g., safely declaring a username creation successful), you need not just ordering but also agreement on when the order is final â€” this is the role of **total order broadcast**.

### Total Order Broadcast

- On a **single CPU core**, operations have a clear total order, but in distributed systems achieving this is challenging.
- Timestamp or sequence number ordering is **weaker** than single-leader replication, since it cannot enforce **uniqueness** under faults.
- Single-leader replication enforces order by funneling all operations through one leaderâ€™s CPU, but this creates scalability and failover issues.
- The distributed systems solution is **total order broadcast** (**atomic broadcast**), which ensures two guarantees:
  - **Reliable delivery** â€“ if one node delivers a message, all nodes do.
  - **Totally ordered delivery** â€“ all nodes see messages in the same order.
- A correct algorithm must uphold these guarantees despite node or network faults, retrying until messages are successfully and consistently delivered.

#### Using total order broadcast

- Consensus systems like *ZooKeeper* and *etcd* implement total order broadcast, highlighting its close link to consensus.
- It underpins several key use cases:
  - **Database replication** (state machine replication): replicas stay consistent if they process the same writes in the same order.
  - **Serializable transactions**: if each message is a deterministic stored procedure executed in the same order, partitions and replicas remain consistent.
  - **Logs**: total order broadcast effectively creates a replication/transaction log, with all nodes appending and reading the same ordered messages.
  - **Lock services** with **fencing** tokens: each lock request is appended to the log and assigned a monotonically increasing sequence number (e.g., ZooKeeperâ€™s *zxid*), ensuring correct fencing.
- A critical property is that once messages are delivered, their order is fixed â€” **no retroactive reordering** is allowedâ€”making this approach stronger than timestamp ordering.

#### Implementing linearizable storage using total order broadcast

- Linearizability and total order broadcast (TOB) are closely related but distinct concepts.
  - TOB guarantees that all messages are delivered reliably and in the **same order** to **all nodes**, but delivery timing is **not guaranteed** (itâ€™s asynchronous).
  - Linearizability guarantees that each read sees the most recent write â€” a stronger recency property.
- You can build linearizable storage (e.g., enforcing unique usernames) on top of TOB by using it as an **append-only log**:
  - Append a tentative claim.
  - Wait until your message is delivered back.
  - If your message is the first for that key, succeed; otherwise, abort.
- Because all nodes receive messages in the same order, they agree on which operation â€œwon.â€ This also enables serializable transactions.
- However, this setup only guarantees **sequential consistency** (operations appear in the same order everywhere) â€” not **full linearizability for reads**.
- To make reads linearizable, systems can:
  - Route reads through the log (like `etcd` quorum reads),
  - Use a sync mechanism to catch up to the latest log position (like *ZooKeeperâ€™s* `sync()`), or
  - Read from a synchronously updated replica (like chain replication).

#### Implementing total order broadcast using linearizable storage

- If you have a linearizable register supporting an atomic **increment-and-get** (or compare-and-set), you can assign each broadcast message a unique, consecutive sequence number. Nodes then deliver messages in order of these sequence numbers, ensuring total ordering.
- Unlike **Lamport timestamps**, these sequence numbers have **no gaps**, so nodes can detect missing messages (e.g., if message 5 is missing between 4 and 6).
- However, maintaining such a linearizable counter in a distributed system is hard â€” **failures** and **network partitions** make it complex to ensure correctness.
- ğŸ‘‰ Linearizable registers, total order broadcast, and consensus are **equivalent problems** â€” solving one allows you to implement the others. This equivalence leads into the study of **consensus algorithms** in the next section.

## Distributed Transactions and Consensus

- Consensus - getting multiple nodes in a distributed system to agree on a single decision - is one of the most fundamental yet difficult problems in distributed computing.
- Although it sounds simple, many systems have failed because the problem was **underestimated** ğŸ¤·â€â™€ï¸. Understanding consensus requires knowledge of topics like replication, transactions, system models, and linearizability.
- Consensus is crucial in scenarios such as:
  - **Leader election**: ensuring all nodes agree on one leader to prevent split-brain situations.
  - **Atomic commit**: ensuring all nodes in a distributed transaction either commit or abort together to preserve atomicity.

### Atomic Commit and Two-Phase Commit (2PC)

- Transaction atomicity ensures that when a transaction involves multiple writes, the system produces a clear, **all-or-nothing** outcome:
  - **Commit**: all writes are successfully applied and made **durable**.
  - **Abort**: all writes are rolled back, leaving **no partial changes**.
- This guarantees that failed transactions donâ€™t leave the database in an **inconsistent** or **half-updated** state. Atomicity is particularly crucial for multi-object transactions and systems with secondary indexes, ensuring those **indexes** remain **consistent** with the **primary data**.

#### From single-node to distributed atomic commit

- For **single-node** transactions, atomicity is handled by the storage engine using a WAL:
  - The node writes all transaction data to disk, then appends a commit record.
  - If a crash occurs before the commit record is written, the transaction aborts; if after, itâ€™s committed.
  - Thus, the moment the commit record is safely written defines the **commit point** â€” the decision is made by one disk controller on one node.
- For **multi-node** transactions, things are more complex:
  - Simply sending commit requests to all nodes can lead to inconsistency â€” some nodes might commit while others abort due to failures, network loss, or conflicts.
  - Once a node commits, it cannot later abort, since other transactions may already depend on that data.
  - â–¶ï¸ Therefore, a node must only commit when itâ€™s certain **all participants will commit**, ensuring atomicity across the system.
- Undoing a committed transaction can only be done via a separate compensating transaction, not by retroactive rollback.

#### Introduction to two-phase commit

- Two-Phase Commit (2PC) is a distributed algorithm that ensures **atomic commits** across **multiple nodes** â€” meaning either all nodes commit or all abort.
- It introduces a **coordinator** (transaction manager) that orchestrates the process among participants (the database nodes). The protocol has two phases.
- Phase 1ï¸âƒ£ â€” Prepare:
  - The coordinator asks each participant if it can commit.
  - Each node responds â€œyesâ€ (ready) or â€œnoâ€ (cannot commit).
- Phase 2ï¸âƒ£ â€” Commit or Abort:
  - If all reply â€œyes,â€ the coordinator sends a commit command.
  - If any reply â€œno,â€ it sends an abort command to all.
- The process ensures consistency but introduces dependency on the coordinator.
<p align="center"><img src="assets/2-phase-commit.png" width="450px" height="auto"></p>

#### A system of promises

- 2PC guarantees atomicity across multiple nodes by introducing **two irreversible commitment points** â€” one for **participants** and one for the **coordinator**.
- Hereâ€™s how it ensures atomicity:
  - The coordinator assigns a **global transaction ID** and manages the process.
  - Each participant runs a local transaction tagged with that ID.
  - When ready to commit, the coordinator sends a prepare request to all participants.
  - Each participant ensures it can commit under all circumstances (**writing all data to disk**, **verifying constraints**) before replying â€œyes.â€ Saying â€œyesâ€ means it cannot abort later, even after crashes â—
  - Once all votes are in, the coordinator makes a final decision (commit or abort), writes it to disk â€” the commit point â€” and then sends the final command to all participants.
  - The coordinator **must retry commit/abort** messages **indefinitely** until all nodes acknowledge.
- Atomicity is preserved because:
  - Participants promise to commit once they vote â€œyes.â€
  - The coordinatorâ€™s decision is final once recorded.
- In short:
  - Saying â€œyesâ€ (prepare phase) = can no longer abort.
  - Coordinator deciding â€œcommitâ€ = can no longer change its mind.

#### Coordinator failure

- In 2PC, if a participant or the network fails, recovery is straightforward through retries or aborts.
- However, if the **coordinator crashes**, the system can enter an uncertain state:
  - Once a participant votes â€œyesâ€ (in the prepare phase), it cannot unilaterally abort â€” it **must wait** for the coordinatorâ€™s final decision.
  - If the coordinator crashes before sending the final commit/abort message, participants that already voted â€œyesâ€ become **in doubt** (uncertain whether to commit or abort).
  - **Acting independently** (e.g., aborting after a timeout) r**isks data inconsistency**, since some participants may have already committed.
- To recover safely, the coordinator must log its decision (commit or abort) to durable storage before notifying participants.
- When it restarts, it reads this log to resolve all in-doubt transactions:
  - Transactions with a commit record â†’ commit.
  - Transactions without one â†’ abort.
- â–¶ï¸ The commit point of 2PC effectively depends on a **single-node** atomic commit at the coordinator ğŸ«¤.

#### Three-phase commit

- 2PC is known as a **blocking** atomic commit protocol because it can get stuck waiting for the coordinator to recover after a failure.
- While **nonblocking alternatives** exist â€” such as **three-phase commit** (3PC) â€” they rely on unrealistic assumptions like **bounded network delay** and **guaranteed response times**.
- In real-world distributed systems, where delays and pauses are unbounded, these assumptions donâ€™t hold, and 3PC cannot ensure atomicity.
- In theory, nonblocking commit requires a **perfect failure detector** (a mechanism that can always tell whether a node has truly failed), but such a detector is impossible to build in unreliable networks ğŸ¤·â€â™‚ï¸.
- Therefore, despite its blocking limitation, 2PC remains the **standard** in practice because itâ€™s the most reliable option under realistic network conditions.

### Distributed Transactions in Practice

- There is a mixed reputation of distributed transactions, particularly those using 2PC:
  - ğŸ‘ They offer **strong safety** guarantees that are otherwise difficult to achieve.
  - ğŸ‘ They often cause **operational complexity**, **poor performance**, and **overpromised reliability**. Many cloud systems avoid them due to these issues ğŸ¤·.
- For example, *MySQL*â€™s distributed transactions can be `10Ã—` slower than single-node ones, mainly because of extra disk syncs (`fsync`) for crash recovery and network round-trips.
- We can distinguish two types of distributed transactions:
  - **Database-internal** transactions:
    - Occur between nodes of the **same distributed database** (e.g., VoltDB, MySQL Cluster NDB).
    - Use a common protocol and can be optimized for that system â€” often **work reasonably well**.
  - **Heterogeneous** distributed transactions:
    - Span across different systems or technologies (e.g., different databases, message brokers).
    - Must ensure atomic commit across incompatible systems â€” much harder to implement reliably.
- ğŸ‘‰ Donâ€™t dismiss distributed transactions entirely â€” study them carefully to understand their trade-offs and lessons.

#### Exactly-once message processing

- Heterogeneous distributed transactions enable atomic coordination across **different systems** â€” for example, ensuring that a **message acknowledgment** and a **database update** either both succeed or both fail.
- This allows powerful guarantees such as **exactly-once** message processing:
  - If the database write or message acknowledgment fails, the whole transaction aborts, and the message can be safely retried.
  - All side effects of a failed transaction are rolled back, ensuring consistency.
- However, this only works if all participating systems support the **same atomic commit protocol** (e.g., two-phase commit).
- If one system (like an email server) doesnâ€™t, it can cause inconsistencies such as duplicate emails on retries.
- ğŸ‘‰ Heterogeneous distributed transactions make strong atomic guarantees across systems â€” but only when every participant supports coordinated commit semantics.

#### XA transactions